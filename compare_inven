#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Compare Prowler Quick Inventory CSVs (ARN-only, hardened):

- Union of per-region CSVs (deduped strictly by resource_arn, after aggressive sanitization)
  vs each ALL-regions CSV.
- If resource_arn is blank in any file, the whole row is excluded from matching
  and logged verbatim into summary.txt.
- Writes a human-readable summary.txt with:
    * total files checked
    * per-file raw line counts & unique ARNs (by sanitized ARN)
    * all ARN-missing rows (full line) per file
    * per ALL file: counts (per_union_unique, all_unique, matched, missing_in_all, extra_in_all)
    * totals across ALL files (note: sums may double-count snapshots)
- Writes Missing/Extra as CSV and TXT (readable tables)
- Adds "NEAR MATCHES" hints in each diff__*.txt to highlight likely formatting mismatches.

Columns tolerated in any order/sep (case/space/punctuation tolerant):
  Account, Region, partition, service, resource type, resource id, resource arn (incl. AWS_ResourceARN), aws tags

================== CONFIGURE HERE ==================
"""

from __future__ import annotations
import argparse
import csv
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# ---------- CONFIG: set your folders here ----------
ALL_INPUT       = r"./inventory/all"          # File OR folder containing ALL-regions CSV(s)
PER_REGION_ROOT = r"./inventory/per_region"   # Folder with per-region CSVs (flat or subfolders)
OUTPUT_DIR      = r"./compare_out"            # Where summary + diffs will be written
FILENAME_FILTER = "prowler"                   # Only consider CSVs whose filename contains this ("" = include all)
ALL_MODE        = "each"                      # If ALL_INPUT is a folder: "latest" or "each"
NEAR_MATCH_MAX  = 3                           # Max suggestions per missing ARN in diff__*.txt
# ---------------------------------------------------

# ===== CLI (optional — you can rely on CONFIG above) =====
def parse_args():
    p = argparse.ArgumentParser("Compare Prowler Quick Inventory (ARN-only, hardened).")
    p.add_argument("--all", default=ALL_INPUT, help="Path to ALL CSV file OR a folder containing ALL CSVs.")
    p.add_argument("--per-root", default=PER_REGION_ROOT, help="Root folder with per-region CSVs.")
    p.add_argument("--output", default=OUTPUT_DIR, help="Output folder for reports (summary + diffs).")
    p.add_argument("--filter", default=FILENAME_FILTER, help="Only consider CSVs whose filenames contain this string.")
    p.add_argument("--all-mode", choices=["latest","each"], default=ALL_MODE,
                   help="If --all is a folder: 'latest' picks newest CSV; 'each' compares every CSV in that folder.")
    p.add_argument("--near-match-max", type=int, default=NEAR_MATCH_MAX,
                   help="Max near-match suggestions per missing ARN in diff__*.txt.")
    return p.parse_args()

# ===== CSV helpers =====
def detect_sep(csv_path: Path) -> str:
    with open(csv_path, "r", encoding="utf-8", errors="replace") as f:
        head = f.readline()
    if ";" in head and "," not in head:
        return ";"
    return ";" if head.count(";") >= head.count(",") else ","

def list_csvs(folder: Path, name_filter: str) -> List[Path]:
    cs = [p for p in folder.rglob("*.csv") if (name_filter in p.name) or (not name_filter)]
    return sorted(cs, key=lambda p: p.stat().st_mtime)

def newest_csv(folder: Path, name_filter: str) -> Optional[Path]:
    cs = list_csvs(folder, name_filter)
    return cs[-1] if cs else None

def load_csv_with_headers(path: Path) -> Tuple[List[Dict[str,str]], List[str]]:
    """Load CSV into list of dicts preserving original header names & order."""
    sep = detect_sep(path)
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        rdr = csv.DictReader(f, delimiter=sep)
        rows = []
        for r in rdr:
            rows.append({k: (v or "").strip() for k, v in r.items()})
        return rows, (rdr.fieldnames or [])

# ===== Normalization & ARN handling =====
# Invisible chars that commonly sneak into CSVs (NBSP, zero-width, BOM, LRM/RLM, word-joiner)
INVIS_RE = re.compile(r"[\u00A0\u200B-\u200D\u200E\u200F\u202A-\u202E\u2060\uFEFF]")

def strip_invis(s: str) -> str:
    return INVIS_RE.sub("", s or "")

def norm_header(h: str) -> str:
    # Remove invisibles, lower, collapse non-alnum to underscore
    return re.sub(r"[^a-z0-9]+", "_", strip_invis(h).strip().lower())

def find_arn_value(row: Dict[str,str]) -> str:
    """
    Robustly locate the ARN column:
      - exact patterns: resource_arn, resource arn, arn, AWS_ResourceARN
      - any header whose normalized form CONTAINS 'resourcearn'
      - any header whose normalized form ENDS WITH 'arn'
    """
    if not row:
        return ""
    # First pass: common names
    candidates = ["resource_arn", "resource arn", "arn", "AWS_ResourceARN"]
    norm_map = {k: norm_header(k) for k in row.keys()}
    for want in candidates:
        nw = norm_header(want)
        for k, nk in norm_map.items():
            if nk == nw:
                return row.get(k, "")
    # Second pass: contains 'resourcearn'
    for k, nk in norm_map.items():
        if "resourcearn" in nk:
            return row.get(k, "")
    # Third pass: endswith 'arn'
    for k, nk in norm_map.items():
        if nk.endswith("arn"):
            return row.get(k, "")
    return ""

def sanitize_arn(raw: str) -> str:
    """
    Aggressively sanitize an ARN for matching:
      - strip invisibles & surrounding quotes
      - strip all whitespace characters
      - lower-case
      - normalize S3 bucket ARNs by removing a single trailing '/'
    """
    s = strip_invis((raw or "").strip())
    # strip outer quotes
    if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
        s = s[1:-1].strip()
    # remove all whitespace chars (spaces, tabs)
    s = re.sub(r"\s+", "", s)
    s = s.lower()

    # normalize s3 bucket arns: arn:aws:s3:::bucket/  -> arn:aws:s3:::bucket
    if s.startswith("arn:aws:s3:::") and s.endswith("/") and s.count("/") == 3:
        s = s[:-1]
    return s

# Map to canonical output schema for reporting
OUT_HEADER = ["account","region","partition","service","resource_type","resource_id","resource_arn","aws_tags"]
def to_schema(row: Dict[str,str]) -> Dict[str,str]:
    def pick(*names: str) -> str:
        for n in names:
            for k in row.keys():
                if norm_header(k) == norm_header(n):
                    return row.get(k, "")
        return ""
    return {
        "account":      pick("account", "account_id"),
        "region":       pick("region"),
        "partition":    pick("partition"),
        "service":      pick("service", "service_name"),
        "resource_type":pick("resource_type", "type", "resource type"),
        "resource_id":  pick("resource_id", "resource id", "id", "name", "resource_name"),
        "resource_arn": pick("resource_arn", "resource arn", "arn", "AWS_ResourceARN"),
        "aws_tags":     pick("aws_tags", "tags"),
    }

def row_to_kv_line(row: Dict[str,str], field_order: List[str]) -> str:
    parts = []
    for h in field_order:
        val = (row.get(h, "") or "").replace("\n", " ").replace("\r", " ")
        parts.append(f"{h}={val}")
    for k in row.keys():
        if k not in field_order:
            val = (row.get(k, "") or "").replace("\n", " ").replace("\r", " ")
            parts.append(f"{k}={val}")
    return " | ".join(parts)

# ===== Output helpers =====
def ensure_dirs(base: Path) -> Dict[str, Path]:
    d = {"root": base, "reports": base / "reports"}
    for v in d.values(): v.mkdir(parents=True, exist_ok=True)
    return d

def write_csv(path: Path, rows: List[Dict[str,str]], header: List[str]):
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=header)
        w.writeheader()
        for r in rows:
            w.writerow({k: r.get(k, "") for k in header})

def write_txt_line(fh, text: str=""):
    fh.write(text + "\n")

def write_txt_table(fh, header: List[str], rows: List[List[str]]):
    widths = [len(h) for h in header]
    for row in rows:
        for i, cell in enumerate(row):
            widths[i] = max(widths[i], len(str(cell)))
    sep = "+".join("-"*(w+2) for w in widths)
    write_txt_line(fh, sep)
    write_txt_line(fh, "| " + " | ".join(h.ljust(widths[i]) for i, h in enumerate(header)) + " |")
    write_txt_line(fh, sep)
    for row in rows:
        write_txt_line(fh, "| " + " | ".join(str(cell).ljust(widths[i]) for i, cell in enumerate(row)) + " |")
    write_txt_line(fh, sep)

def rows_to_txt_block(rows: List[Dict[str,str]], extra_cols: List[str]=None) -> Tuple[List[str], List[List[str]]]:
    cols = OUT_HEADER + (extra_cols or [])
    table_rows = []
    for r in rows:
        table_rows.append([r.get(c,"") for c in cols])
    return cols, table_rows

# ===== Ingest per-region (ARN-only) =====
def list_per_region_csvs(per_root: Path, name_filter: str) -> List[Path]:
    out = []
    for p in per_root.rglob("*.csv"):
        if any(part in {"logs","reports","normalized"} for part in p.parts):
            continue
        if name_filter and (name_filter not in p.name):
            continue
        out.append(p)
    return sorted(out, key=lambda p: p.stat().st_mtime)

def build_per_union(per_root: Path, name_filter: str):
    """
    Returns:
      per_union_rows: list[dict] of normalized rows for unique sanitized ARNs (provenance _source_file)
      per_files_stats: list of per-file counters
      arn_missing_log: [{source_file, lines:[str,...]}]
      per_files: list of included file Paths
    """
    per_files = list_per_region_csvs(per_root, name_filter)
    union_by_arn: Dict[str, Dict[str,str]] = {}
    per_files_stats: List[Dict[str, str|int]] = []
    arn_missing_log: List[Dict[str, object]] = []

    for f in per_files:
        rows_raw, field_order = load_csv_with_headers(f)
        rows_read = len(rows_raw)
        unique_arn_in_file = 0
        seen_arns_in_file = set()
        arn_missing_lines: List[str] = []

        for raw in rows_raw:
            arn_raw = find_arn_value(raw)
            if not arn_raw or not arn_raw.strip():
                arn_missing_lines.append(row_to_kv_line(raw, field_order))
                continue
            arn_key = sanitize_arn(arn_raw)
            if not arn_key:
                arn_missing_lines.append(row_to_kv_line(raw, field_order))
                continue
            if arn_key not in seen_arns_in_file:
                seen_arns_in_file.add(arn_key)
                unique_arn_in_file += 1
            if arn_key not in union_by_arn:
                row_std = to_schema(raw)
                row_std["_source_file"] = f.name
                union_by_arn[arn_key] = row_std

        per_files_stats.append({
            "source_file": f.name,
            "rows_read": rows_read,
            "unique_arns_in_file": unique_arn_in_file,
            "arn_missing_in_file": len(arn_missing_lines),
        })
        if arn_missing_lines:
            arn_missing_log.append({"source_file": f.name, "lines": arn_missing_lines})

    return list(union_by_arn.values()), per_files_stats, arn_missing_log, per_files

# ===== Ingest ALL files (ARN-only) =====
def resolve_all_csvs(all_input: Path, name_filter: str, mode: str) -> List[Path]:
    if all_input.is_file():
        return [all_input]
    if all_input.is_dir():
        if mode == "latest":
            pick = newest_csv(all_input, name_filter)
            return [pick] if pick else []
        return list_csvs(all_input, name_filter)
    return []

def dedupe_all_by_arn(rows_raw: List[Dict[str,str]]) -> Tuple[List[Dict[str,str]], int, List[str], List[str]]:
    """
    Return:
      all_unique_rows (by sanitized ARN),
      rows_read_count,
      arn_missing_lines (full-line strings),
      all_arns_sanitized (list of sanitized arns from ALL file for near-match search)
    """
    rows_read = len(rows_raw)
    uniq_by_arn: Dict[str, Dict[str,str]] = {}
    arn_missing_lines: List[str] = []
    all_sanitized_arns: List[str] = []

    field_order = list(rows_raw[0].keys()) if rows_raw else []

    for raw in rows_raw:
        arn_raw = find_arn_value(raw)
        if not arn_raw or not arn_raw.strip():
            arn_missing_lines.append(row_to_kv_line(raw, field_order))
            continue
        arn_key = sanitize_arn(arn_raw)
        if not arn_key:
            arn_missing_lines.append(row_to_kv_line(raw, field_order))
            continue
        if arn_key not in uniq_by_arn:
            uniq_by_arn[arn_key] = to_schema(raw)
        all_sanitized_arns.append(arn_key)

    return list(uniq_by_arn.values()), rows_read, arn_missing_lines, all_sanitized_arns

# ===== Diff & near-match =====
def find_near_matches(missing_arn_key: str, all_arn_keys: List[str], max_hits: int = 3) -> List[str]:
    """
    Heuristic near-match finder for diagnostics (not used for matching logic):
      - exact with/without single trailing '/': s or s + '/'
      - same tail token (after last ':' or '/' in ARN resource part)
      - contains-substring match (case-insensitive) for a middle token
    Returns list of ARN strings (sanitized form) from the ALL file.
    """
    hits: List[str] = []

    # variant: with/without trailing slash
    if missing_arn_key.endswith("/"):
        alt = missing_arn_key[:-1]
    else:
        alt = missing_arn_key + "/"
    if alt in all_arn_keys:
        hits.append(alt)
        if len(hits) >= max_hits:
            return hits

    # extract a tail token from resource part
    # arn:part:svc:region:acct:resource
    parts = missing_arn_key.split(":", 5)
    resource = parts[5] if len(parts) >= 6 else ""
    tail = resource.split("/")[-1].split(":")[-1]
    if tail and len(tail) >= 3:
        for cand in all_arn_keys:
            if tail in cand:
                if cand not in hits:
                    hits.append(cand)
                    if len(hits) >= max_hits:
                        return hits

    # broad substring (only if still nothing)
    mid = tail or (resource[:20] if resource else missing_arn_key[:20])
    if mid:
        for cand in all_arn_keys:
            if mid in cand:
                if cand not in hits:
                    hits.append(cand)
                    if len(hits) >= max_hits:
                        return hits

    return hits

def write_diff_files(per_union: List[Dict[str,str]],
                     all_rows: List[Dict[str,str]],
                     all_file_name: str,
                     out_reports: Path,
                     all_sanitized_arns: List[str],
                     near_match_max: int) -> Dict[str,int]:
    # Sets by sanitized ARN only
    per_arns = { sanitize_arn(r.get("resource_arn","")) for r in per_union if r.get("resource_arn") }
    all_arns = { sanitize_arn(r.get("resource_arn","")) for r in all_rows if r.get("resource_arn") }
    inter = per_arns & all_arns

    # Dicts by arn for reporting
    per_by_arn = { sanitize_arn(r["resource_arn"]): r for r in per_union if r.get("resource_arn") }
    all_by_arn = { sanitize_arn(r["resource_arn"]): r for r in all_rows if r.get("resource_arn") }

    # Missing & Extra
    missing = []
    for arn_key in sorted(per_arns - all_arns):
        r = dict(per_by_arn[arn_key])
        r["source_file"] = r.pop("_source_file", r.get("source_file",""))
        missing.append(r)

    extra = []
    for arn_key in sorted(all_arns - per_arns):
        r = dict(all_by_arn[arn_key])
        r["source_all_file"] = all_file_name
        extra.append(r)

    # Write CSV
    stem = Path(all_file_name).stem
    missing_csv = out_reports / f"missing_in_all__{stem}.csv"
    extra_csv   = out_reports / f"extra_in_all__{stem}.csv"
    write_csv(missing_csv, missing, OUT_HEADER + ["source_file"])
    write_csv(extra_csv,   extra,   OUT_HEADER + ["source_all_file"])

    # Write TXT with near-match hints
    diff_txt = out_reports / f"diff__{stem}.txt"
    with open(diff_txt, "w", encoding="utf-8") as fh:
        write_txt_line(fh, f"DIFF for ALL file: {all_file_name}")
        write_txt_line(fh, f"Generated: {datetime.utcnow().isoformat()}Z")
        write_txt_line(fh, "")

        # Missing section
        write_txt_line(fh, f"[MISSING in ALL] count={len(missing)}")
        if missing:
            header, rows = rows_to_txt_block(missing, ["source_file"])
            write_txt_table(fh, header, rows)
            write_txt_line(fh, "")
            # Near matches
            write_txt_line(fh, "[NEAR MATCH HINTS for missing ARNs]")
            for m in missing:
                mk = sanitize_arn(m.get("resource_arn",""))
                hints = find_near_matches(mk, all_sanitized_arns, max_hits=near_match_max)
                if hints:
                    write_txt_line(fh, f"  ARN: {m.get('resource_arn','')}  -> suggestions:")
                    for h in hints:
                        write_txt_line(fh, f"    {h}")
                else:
                    write_txt_line(fh, f"  ARN: {m.get('resource_arn','')}  -> no near matches found")
        else:
            write_txt_line(fh, "(none)")
        write_txt_line(fh, "")

        # Extra section
        write_txt_line(fh, f"[EXTRA in ALL] count={len(extra)}")
        if extra:
            header, rows = rows_to_txt_block(extra, ["source_all_file"])
            write_txt_table(fh, header, rows)
        else:
            write_txt_line(fh, "(none)")

    return {
        "per_union_unique": len(per_arns),
        "all_unique": len(all_arns),
        "matched": len(inter),
        "missing_in_all": len(missing),
        "extra_in_all": len(extra),
    }

# ===== Main =====
def main():
    args = parse_args()
    per_root = Path(args.per_root)
    all_input = Path(args.all)
    out_dirs = ensure_dirs(Path(args.output).resolve())

    if not per_root.exists():
        raise SystemExit(f"[ERROR] --per-root not found: {per_root}")
    if not all_input.exists():
        raise SystemExit(f"[ERROR] --all not found: {all_input}")

    # Build per-region union (ARN-only, sanitized)
    per_union, per_file_stats, per_arn_missing_log, per_files = build_per_union(per_root, args.filter)

    # Resolve ALL files
    if all_input.is_file():
        all_files = [all_input]
    else:
        all_files = resolve_all_csvs(all_input, args.filter, args.all_mode)
    if not all_files:
        raise SystemExit(f"[ERROR] No ALL CSVs found in: {all_input} (filter='{args.filter}', mode={args.all_mode})")

    # Compare each ALL file
    all_file_counts: List[Dict[str, str|int]] = []
    all_file_line_stats: List[Dict[str, str|int]] = []
    all_arn_missing_log: List[Dict[str, object]] = []

    for af in all_files:
        rows_raw, field_order = load_csv_with_headers(af)
        all_unique_rows, all_rows_read, arn_missing_lines, all_sanitized_arns = dedupe_all_by_arn(rows_raw)

        all_file_line_stats.append({
            "all_file": af.name,
            "rows_read": all_rows_read,
            "unique_arns_in_file": len(all_unique_rows),
            "arn_missing_in_file": len(arn_missing_lines),
        })
        if arn_missing_lines:
            all_arn_missing_log.append({"all_file": af.name, "lines": arn_missing_lines})

        counts = write_diff_files(
            per_union, all_unique_rows, af.name, out_dirs["reports"],
            all_sanitized_arns=all_sanitized_arns,
            near_match_max=args.near_match_max
        )
        counts["all_file"] = af.name
        all_file_counts.append(counts)

    # ===== summary.txt =====
    summary_txt = out_dirs["reports"] / "summary.txt"
    with open(summary_txt, "w", encoding="utf-8") as fh:
        write_txt_line(fh, "Prowler Quick Inventory — ARN-only Comparison Summary (hardened)")
        write_txt_line(fh, f"Generated: {datetime.utcnow().isoformat()}Z")
        write_txt_line(fh, "")

        write_txt_line(fh, "[INPUTS]")
        write_txt_line(fh, f"Per-region root : {per_root}")
        write_txt_line(fh, f"ALL input       : {all_input}  (mode={args.all_mode})")
        write_txt_line(fh, f"Filename filter : '{args.filter}'")
        write_txt_line(fh, "")

        # Per-region files
        write_txt_line(fh, f"[PER-REGION FILES] total_files={len(per_files)}")
        pr_header = ["source_file","rows_read","unique_arns_in_file","arn_missing_in_file"]
        pr_rows = [[s[h] for h in pr_header] for s in per_file_stats]
        write_txt_table(fh, pr_header, pr_rows)
        write_txt_line(fh, "")

        # ALL files (line counts)
        write_txt_line(fh, f"[ALL FILES] total_files={len(all_files)}")
        af_header = ["all_file","rows_read","unique_arns_in_file","arn_missing_in_file"]
        af_rows = [[s[h] for h in af_header] for s in all_file_line_stats]
        write_txt_table(fh, af_header, af_rows)
        write_txt_line(fh, "")

        # Per ALL file comparison counts
        write_txt_line(fh, "[COMPARISON COUNTS per ALL file] (ARN-only)")
        cmp_header = ["all_file","per_union_unique","all_unique","matched","missing_in_all","extra_in_all"]
        cmp_rows = [[c[h] for h in cmp_header] for c in all_file_counts]
        write_txt_table(fh, cmp_header, cmp_rows)
        write_txt_line(fh, "")

        # Totals across ALL files (sums may double-count if snapshots overlap)
        total_per_union_once = all_file_counts[0]["per_union_unique"] if all_file_counts else 0
        sum_all_unique = sum(int(c["all_unique"]) for c in all_file_counts)
        sum_matched = sum(int(c["matched"]) for c in all_file_counts)
        sum_missing = sum(int(c["missing_in_all"]) for c in all_file_counts)
        sum_extra = sum(int(c["extra_in_all"]) for c in all_file_counts)
        write_txt_line(fh, "[TOTALS across ALL files (sums may double-count if files overlap)]")
        tot_header = ["per_union_unique_once","sum_all_unique","sum_matched","sum_missing_in_all","sum_extra_in_all"]
        tot_rows = [[total_per_union_once, sum_all_unique, sum_matched, sum_missing, sum_extra]]
        write_txt_table(fh, tot_header, tot_rows)
        write_txt_line(fh, "")

        # ARN-missing rows (Per-region)
        write_txt_line(fh, "[ARN-MISSING ROWS IN PER-REGION FILES] (excluded from matching)")
        if not per_arn_missing_log:
            write_txt_line(fh, "(none)")
        else:
            for block in per_arn_missing_log:
                write_txt_line(fh, f"  File: {block['source_file']}  ({len(block['lines'])} rows)")
                for ln in block["lines"]:
                    write_txt_line(fh, "    " + ln)
        write_txt_line(fh, "")

        # ARN-missing rows (ALL files)
        write_txt_line(fh, "[ARN-MISSING ROWS IN ALL FILES] (excluded from matching)")
        if not all_arn_missing_log:
            write_txt_line(fh, "(none)")
        else:
            for block in all_arn_missing_log:
                write_txt_line(fh, f"  File: {block['all_file']}  ({len(block['lines'])} rows)")
                for ln in block["lines"]:
                    write_txt_line(fh, "    " + ln)
        write_txt_line(fh, "")

        # Pointers to diff artifacts
        write_txt_line(fh, "[OUTPUT ARTIFACTS]")
        for c in all_file_counts:
            stem = Path(c["all_file"]).stem
            write_txt_line(fh, f"  Missing CSV : {out_dirs['reports'] / ('missing_in_all__' + stem + '.csv')}")
            write_txt_line(fh, f"  Extra   CSV : {out_dirs['reports'] / ('extra_in_all__' + stem + '.csv')}")
            write_txt_line(fh, f"  Diff TXT    : {out_dirs['reports'] / ('diff__' + stem + '.txt')}")
        write_txt_line(fh, "")

    print(f"[OK] Summary written: {summary_txt}")
    for c in all_file_counts:
        stem = Path(c["all_file"]).stem
        print(f"[OK] Missing CSV: {out_dirs['reports'] / ('missing_in_all__' + stem + '.csv')}")
        print(f"[OK] Extra   CSV: {out_dirs['reports'] / ('extra_in_all__' + stem + '.csv')}")
        print(f"[OK] Diff TXT   : {out_dirs['reports'] / ('diff__' + stem + '.txt')}")

if __name__ == "__main__":
    main()
